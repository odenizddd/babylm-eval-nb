{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/odenizddd/babylm-eval-nb/blob/main/BabyLM_Evaluation_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfxV13o02TjF"
      },
      "source": [
        "copy necessary files from huggingface\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /\n",
        "\n",
        "!rm -rf /content/evaluation-pipeline-2023\n",
        "!rm -rf /content/elc-bert-replica"
      ],
      "metadata": {
        "id": "q54tboEV1rOi"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slJ3LaDE1Q0H",
        "outputId": "63baed29-ae63-4245-8a02-524c4fc9c284"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "git-lfs is already the newest version (3.0.2-1ubuntu0.3).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 34 not upgraded.\n",
            "Git LFS initialized.\n",
            "Cloning into 'elc-bert-replica'...\n",
            "remote: Enumerating objects: 18, done.\u001b[K\n",
            "remote: Counting objects: 100% (15/15), done.\u001b[K\n",
            "remote: Compressing objects: 100% (15/15), done.\u001b[K\n",
            "remote: Total 18 (delta 3), reused 0 (delta 0), pack-reused 3 (from 1)\u001b[K\n",
            "Unpacking objects: 100% (18/18), 50.87 KiB | 4.24 MiB/s, done.\n",
            "Filtering content: 100% (2/2), 592.96 MiB | 122.27 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "!apt-get install git-lfs\n",
        "!git lfs install\n",
        "!git clone https://huggingface.co/odenizddd/elc-bert-replica\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRGWKaudsoSu"
      },
      "source": [
        "# Instructions\n",
        "This notebook allows you to load and evaluate a huggingface model on a subset of BLiMP (a linguistic acceptability judgment dataset) and GLUE (a natural language understanding benchmark collection). It is HIGHLY recommended to clone the GitHub repository and evaluate your model in the command-line; this will give you more freedom in the kinds of models you can evaluate. However, Colab provides a GPU that will allow you to load and evaluate smaller models.\n",
        "\n",
        "To use this notebook:\n",
        "\n",
        "1. Start by making a copy of this notebook so that you can make edits and run the code: File > Save a copy in Drive.\n",
        "\n",
        "2. Set Runtime > Change runtime type > Hardware accelerator to GPU if it isn't already.\n",
        "\n",
        "3. Run the setup script to install the required packages for evaluating.\n",
        "\n",
        "4. Upload your model to the colab in the `/content/model_folder/` directory. This folder should include the following files, and probably a couple more depending on the type of model and tokenizer you use:\n",
        "* `config.json`\n",
        "* `pytorch_model.bin`\n",
        "* `tokenizer_config.json`\n",
        "* `vocab.json`\n",
        "\n",
        "  a. To obtain these files given your pre-trained model and your tokenizer, load them using huggingface `transformers` and save them using these commands:\n",
        "```\n",
        "tokenizer.save_pretrained(\"./model_dir\")\n",
        "model.save_pretrained(\"./model_dir\")\n",
        "```\n",
        "  b. Then, upload all the contents of `model_dir` (including any other files not mentioned above) to the `model_folder` folder in this Colab.\n",
        "\n",
        "5. Choose the proper model type in the dropdown in the \"load model and evaluate\" cell. Use \"decoder\" for autoregressive (sometimes called \"causal\") language models, like GPT/OPT; \"encoder\" for masked language models, like BERT/RoBERTa; or \"encoder-decoder\" for text-to-text models, like T5/BART.\n",
        "\n",
        "6. Run the cells below to load and evaluate your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HbPFbtYqMREj",
        "outputId": "6a366555-8dd3-453e-f938-026b7f340f44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Note, selecting 'python3-distutils' instead of 'python3.10-distutils'\n",
            "python3-distutils is already the newest version (3.10.8-1~22.04).\n",
            "The following additional packages will be installed:\n",
            "  python3-pip-whl python3-setuptools-whl\n",
            "The following NEW packages will be installed:\n",
            "  python3-pip-whl python3-setuptools-whl python3.10-venv\n",
            "0 upgraded, 3 newly installed, 0 to remove and 34 not upgraded.\n",
            "Need to get 2,474 kB of archives.\n",
            "After this operation, 2,885 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 python3-pip-whl all 22.0.2+dfsg-1ubuntu0.5 [1,680 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 python3-setuptools-whl all 59.6.0-1.2ubuntu0.22.04.2 [788 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 python3.10-venv amd64 3.10.12-1~22.04.9 [5,722 B]\n",
            "Fetched 2,474 kB in 1s (1,707 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package python3-pip-whl.\n",
            "(Reading database ... 126102 files and directories currently installed.)\n",
            "Preparing to unpack .../python3-pip-whl_22.0.2+dfsg-1ubuntu0.5_all.deb ...\n",
            "Unpacking python3-pip-whl (22.0.2+dfsg-1ubuntu0.5) ...\n",
            "Selecting previously unselected package python3-setuptools-whl.\n",
            "Preparing to unpack .../python3-setuptools-whl_59.6.0-1.2ubuntu0.22.04.2_all.deb ...\n",
            "Unpacking python3-setuptools-whl (59.6.0-1.2ubuntu0.22.04.2) ...\n",
            "Selecting previously unselected package python3.10-venv.\n",
            "Preparing to unpack .../python3.10-venv_3.10.12-1~22.04.9_amd64.deb ...\n",
            "Unpacking python3.10-venv (3.10.12-1~22.04.9) ...\n",
            "Setting up python3-setuptools-whl (59.6.0-1.2ubuntu0.22.04.2) ...\n",
            "Setting up python3-pip-whl (22.0.2+dfsg-1ubuntu0.5) ...\n",
            "Setting up python3.10-venv (3.10.12-1~22.04.9) ...\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 2225k  100 2225k    0     0  8702k      0 --:--:-- --:--:-- --:--:-- 8694k\n",
            "Collecting pip\n",
            "  Downloading pip-25.1.1-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting setuptools\n",
            "  Downloading setuptools-80.8.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting wheel\n",
            "  Downloading wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Downloading pip-25.1.1-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setuptools-80.8.0-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wheel-0.45.1-py3-none-any.whl (72 kB)\n",
            "Installing collected packages: wheel, setuptools, pip\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [pip]\n",
            "\u001b[1A\u001b[2KSuccessfully installed pip-25.1.1 setuptools-80.8.0 wheel-0.45.1\n"
          ]
        }
      ],
      "source": [
        "!sudo apt-get install python3.10-venv python3.10-distutils -y\n",
        "!curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py\n",
        "!python3.10 get-pip.py\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lk4Epwozk_sf",
        "outputId": "8f1cc544-dd48-4b2e-e906-e474758c219f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: lm_eval 0.2.0\n",
            "Uninstalling lm_eval-0.2.0:\n",
            "  Successfully uninstalled lm_eval-0.2.0\n",
            "Obtaining file:///content/evaluation-pipeline-2023\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from lm_eval==0.2.0) (3.6.0)\n",
            "Requirement already satisfied: nltk==3.6 in /usr/local/lib/python3.10/dist-packages (from lm_eval==0.2.0) (3.6)\n",
            "Requirement already satisfied: openai==0.13.0 in /usr/local/lib/python3.10/dist-packages (from lm_eval==0.2.0) (0.13.0)\n",
            "Requirement already satisfied: pycountry==20.7.3 in /usr/local/lib/python3.10/dist-packages (from lm_eval==0.2.0) (20.7.3)\n",
            "Requirement already satisfied: pytablewriter==0.58.0 in /usr/local/lib/python3.10/dist-packages (from lm_eval==0.2.0) (0.58.0)\n",
            "Requirement already satisfied: rouge-score==0.0.4 in /usr/local/lib/python3.10/dist-packages (from lm_eval==0.2.0) (0.0.4)\n",
            "Requirement already satisfied: sacrebleu==1.5.0 in /usr/local/lib/python3.10/dist-packages (from lm_eval==0.2.0) (1.5.0)\n",
            "Requirement already satisfied: scikit-learn>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from lm_eval==0.2.0) (1.6.1)\n",
            "Requirement already satisfied: sqlitedict==1.6.0 in /usr/local/lib/python3.10/dist-packages (from lm_eval==0.2.0) (1.6.0)\n",
            "Requirement already satisfied: torch==1.13.0 in /usr/local/lib/python3.10/dist-packages (from lm_eval==0.2.0) (1.13.0)\n",
            "Requirement already satisfied: evaluate==0.4.0 in /usr/local/lib/python3.10/dist-packages (from lm_eval==0.2.0) (0.4.0)\n",
            "Requirement already satisfied: tqdm-multiprocess==0.0.11 in /usr/local/lib/python3.10/dist-packages (from lm_eval==0.2.0) (0.0.11)\n",
            "Collecting accelerate@ git+https://github.com/huggingface/accelerate@v0.20.0 (from lm_eval==0.2.0)\n",
            "  Cloning https://github.com/huggingface/accelerate (to revision v0.20.0) to /tmp/pip-install-g7tiv9i1/accelerate_d57825370d2e43c3a21923f4b402908e\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/accelerate /tmp/pip-install-g7tiv9i1/accelerate_d57825370d2e43c3a21923f4b402908e\n",
            "  Running command git checkout -q a6418cac4f2123d2ad4590cbf5204b9fcb0e3f8f\n",
            "  Resolved https://github.com/huggingface/accelerate to commit a6418cac4f2123d2ad4590cbf5204b9fcb0e3f8f\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers==4.26.1 in /usr/local/lib/python3.10/dist-packages (from lm_eval==0.2.0) (4.26.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate@ git+https://github.com/huggingface/accelerate@v0.20.0->lm_eval==0.2.0) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate@ git+https://github.com/huggingface/accelerate@v0.20.0->lm_eval==0.2.0) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate@ git+https://github.com/huggingface/accelerate@v0.20.0->lm_eval==0.2.0) (7.0.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate@ git+https://github.com/huggingface/accelerate@v0.20.0->lm_eval==0.2.0) (6.0.2)\n",
            "Requirement already satisfied: black<=21.12b0 in /usr/local/lib/python3.10/dist-packages (from lm_eval==0.2.0) (21.12b0)\n",
            "Requirement already satisfied: coverage<=6.2 in /usr/local/lib/python3.10/dist-packages (from lm_eval==0.2.0) (6.2)\n",
            "Requirement already satisfied: mock>=4.0.3 in /usr/local/lib/python3.10/dist-packages (from lm_eval==0.2.0) (5.2.0)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.10/dist-packages (from lm_eval==0.2.0) (8.3.5)\n",
            "Collecting promptsource@ git+https://github.com/bigscience-workshop/promptsource@eval-hackathon (from lm_eval==0.2.0)\n",
            "  Cloning https://github.com/bigscience-workshop/promptsource (to revision eval-hackathon) to /tmp/pip-install-g7tiv9i1/promptsource_b5187fc017ed4d0eb4a011ba0de2a0a1\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/bigscience-workshop/promptsource /tmp/pip-install-g7tiv9i1/promptsource_b5187fc017ed4d0eb4a011ba0de2a0a1\n",
            "  Running command git checkout -b eval-hackathon --track origin/eval-hackathon\n",
            "  Switched to a new branch 'eval-hackathon'\n",
            "  Branch 'eval-hackathon' set up to track remote branch 'eval-hackathon' from 'origin'.\n",
            "  Resolved https://github.com/bigscience-workshop/promptsource to commit e3a22e09d0131a6ca6810ad8684c59eab3ede13d\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: flake8 in /usr/local/lib/python3.10/dist-packages (from promptsource@ git+https://github.com/bigscience-workshop/promptsource@eval-hackathon->lm_eval==0.2.0) (7.2.0)\n",
            "Requirement already satisfied: isort==5.8.0 in /usr/local/lib/python3.10/dist-packages (from promptsource@ git+https://github.com/bigscience-workshop/promptsource@eval-hackathon->lm_eval==0.2.0) (5.8.0)\n",
            "Requirement already satisfied: streamlit==0.82 in /usr/local/lib/python3.10/dist-packages (from promptsource@ git+https://github.com/bigscience-workshop/promptsource@eval-hackathon->lm_eval==0.2.0) (0.82.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from promptsource@ git+https://github.com/bigscience-workshop/promptsource@eval-hackathon->lm_eval==0.2.0) (3.1.6)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from promptsource@ git+https://github.com/bigscience-workshop/promptsource@eval-hackathon->lm_eval==0.2.0) (6.1.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from promptsource@ git+https://github.com/bigscience-workshop/promptsource@eval-hackathon->lm_eval==0.2.0) (2.32.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from promptsource@ git+https://github.com/bigscience-workshop/promptsource@eval-hackathon->lm_eval==0.2.0) (2.2.3)\n",
            "Requirement already satisfied: py7zr in /usr/local/lib/python3.10/dist-packages (from promptsource@ git+https://github.com/bigscience-workshop/promptsource@eval-hackathon->lm_eval==0.2.0) (0.22.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate==0.4.0->lm_eval==0.2.0) (0.3.8)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate==0.4.0->lm_eval==0.2.0) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate==0.4.0->lm_eval==0.2.0) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate==0.4.0->lm_eval==0.2.0) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate==0.4.0->lm_eval==0.2.0) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate==0.4.0->lm_eval==0.2.0) (0.31.4)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from evaluate==0.4.0->lm_eval==0.2.0) (0.18.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk==3.6->lm_eval==0.2.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk==3.6->lm_eval==0.2.0) (1.5.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from nltk==3.6->lm_eval==0.2.0) (2024.11.6)\n",
            "Requirement already satisfied: pandas-stubs>=1.1.0.11 in /usr/local/lib/python3.10/dist-packages (from openai==0.13.0->lm_eval==0.2.0) (2.2.3.250308)\n",
            "Requirement already satisfied: openpyxl>=3.0.7 in /usr/local/lib/python3.10/dist-packages (from openai==0.13.0->lm_eval==0.2.0) (3.1.5)\n",
            "Requirement already satisfied: setuptools>=38.3.0 in /usr/local/lib/python3.10/dist-packages (from pytablewriter==0.58.0->lm_eval==0.2.0) (80.8.0)\n",
            "Requirement already satisfied: DataProperty<2,>=0.50.0 in /usr/local/lib/python3.10/dist-packages (from pytablewriter==0.58.0->lm_eval==0.2.0) (1.1.0)\n",
            "Requirement already satisfied: mbstrdecoder<2,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytablewriter==0.58.0->lm_eval==0.2.0) (1.1.4)\n",
            "Requirement already satisfied: msgfy<1,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from pytablewriter==0.58.0->lm_eval==0.2.0) (0.2.1)\n",
            "Requirement already satisfied: pathvalidate<3,>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from pytablewriter==0.58.0->lm_eval==0.2.0) (2.5.2)\n",
            "Requirement already satisfied: tabledata<2,>=1.1.3 in /usr/local/lib/python3.10/dist-packages (from pytablewriter==0.58.0->lm_eval==0.2.0) (1.3.4)\n",
            "Requirement already satisfied: tcolorpy<1,>=0.0.5 in /usr/local/lib/python3.10/dist-packages (from pytablewriter==0.58.0->lm_eval==0.2.0) (0.1.7)\n",
            "Requirement already satisfied: typepy<2,>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from typepy[datetime]<2,>=1.1.1->pytablewriter==0.58.0->lm_eval==0.2.0) (1.3.4)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score==0.0.4->lm_eval==0.2.0) (2.2.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/lib/python3/dist-packages (from rouge-score==0.0.4->lm_eval==0.2.0) (1.16.0)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu==1.5.0->lm_eval==0.2.0) (3.1.1)\n",
            "Requirement already satisfied: altair>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from streamlit==0.82->promptsource@ git+https://github.com/bigscience-workshop/promptsource@eval-hackathon->lm_eval==0.2.0) (5.5.0)\n",
            "Requirement already satisfied: astor in /usr/local/lib/python3.10/dist-packages (from streamlit==0.82->promptsource@ git+https://github.com/bigscience-workshop/promptsource@eval-hackathon->lm_eval==0.2.0) (0.8.1)\n",
            "Requirement already satisfied: base58 in /usr/local/lib/python3.10/dist-packages (from streamlit==0.82->promptsource@ git+https://github.com/bigscience-workshop/promptsource@eval-hackathon->lm_eval==0.2.0) (2.1.1)\n",
            "Requirement already satisfied: blinker in /usr/lib/python3/dist-packages (from streamlit==0.82->promptsource@ git+https://github.com/bigscience-workshop/promptsource@eval-hackathon->lm_eval==0.2.0) (1.4)\n",
            "Requirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit==0.82->promptsource@ git+https://github.com/bigscience-workshop/promptsource@eval-hackathon->lm_eval==0.2.0) (5.5.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from streamlit==0.82->promptsource@ git+https://github.com/bigscience-workshop/promptsource@eval-hackathon->lm_eval==0.2.0) (11.2.1)\n",
            "Requirement already satisfied: protobuf!=3.11,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from streamlit==0.82->promptsource@ git+https://github.com/bigscience-workshop/promptsource@eval-hackathon->lm_eval==0.2.0) (6.31.0)\n",
            "Requirement already satisfied: pydeck>=0.1.dev5 in /usr/local/lib/python3.10/dist-packages (from streamlit==0.82->promptsource@ git+https://github.com/bigscience-workshop/promptsource@eval-hackathon->lm_eval==0.2.0) (0.9.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from streamlit==0.82->promptsource@ git+https://github.com/bigscience-workshop/promptsource@eval-hackathon->lm_eval==0.2.0) (2.9.0.post0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from streamlit==0.82->promptsource@ git+https://github.com/bigscience-workshop/promptsource@eval-hackathon->lm_eval==0.2.0) (0.10.2)\n",
            "Requirement already satisfied: tornado>=5.0 in /usr/local/lib/python3.10/dist-packages (from streamlit==0.82->promptsource@ git+https://github.com/bigscience-workshop/promptsource@eval-hackathon->lm_eval==0.2.0) (6.5)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.10/dist-packages (from streamlit==0.82->promptsource@ git+https://github.com/bigscience-workshop/promptsource@eval-hackathon->lm_eval==0.2.0) (5.3.1)\n",
            "Requirement already satisfied: validators in /usr/local/lib/python3.10/dist-packages (from streamlit==0.82->promptsource@ git+https://github.com/bigscience-workshop/promptsource@eval-hackathon->lm_eval==0.2.0) (0.35.0)\n",
            "Requirement already satisfied: gitpython in /usr/local/lib/python3.10/dist-packages (from streamlit==0.82->promptsource@ git+https://github.com/bigscience-workshop/promptsource@eval-hackathon->lm_eval==0.2.0) (3.1.44)\n",
            "Requirement already satisfied: watchdog in /usr/local/lib/python3.10/dist-packages (from streamlit==0.82->promptsource@ git+https://github.com/bigscience-workshop/promptsource@eval-hackathon->lm_eval==0.2.0) (6.0.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.13.0->lm_eval==0.2.0) (4.13.2)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.0->lm_eval==0.2.0) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.0->lm_eval==0.2.0) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.0->lm_eval==0.2.0) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.0->lm_eval==0.2.0) (11.7.99)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.0->lm_eval==0.2.0) (0.45.1)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from tqdm-multiprocess==0.0.11->lm_eval==0.2.0) (0.4.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.26.1->lm_eval==0.2.0) (3.18.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.26.1->lm_eval==0.2.0) (0.13.3)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.10/dist-packages (from black<=21.12b0->lm_eval==0.2.0) (4.3.8)\n",
            "Requirement already satisfied: tomli<2.0.0,>=0.2.6 in /usr/local/lib/python3.10/dist-packages (from black<=21.12b0->lm_eval==0.2.0) (1.2.3)\n",
            "Requirement already satisfied: pathspec<1,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from black<=21.12b0->lm_eval==0.2.0) (0.12.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from black<=21.12b0->lm_eval==0.2.0) (1.1.0)\n",
            "Requirement already satisfied: chardet<6,>=3.0.4 in /usr/local/lib/python3.10/dist-packages (from mbstrdecoder<2,>=1.0.0->pytablewriter==0.58.0->lm_eval==0.2.0) (5.2.0)\n",
            "Requirement already satisfied: urllib3>=1.25.10 in /usr/local/lib/python3.10/dist-packages (from responses<0.19->evaluate==0.4.0->lm_eval==0.2.0) (2.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->promptsource@ git+https://github.com/bigscience-workshop/promptsource@eval-hackathon->lm_eval==0.2.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->promptsource@ git+https://github.com/bigscience-workshop/promptsource@eval-hackathon->lm_eval==0.2.0) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->promptsource@ git+https://github.com/bigscience-workshop/promptsource@eval-hackathon->lm_eval==0.2.0) (2025.4.26)\n",
            "Requirement already satisfied: pytz>=2018.9 in /usr/local/lib/python3.10/dist-packages (from typepy[datetime]<2,>=1.1.1->pytablewriter==0.58.0->lm_eval==0.2.0) (2025.2)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair>=3.2.0->streamlit==0.82->promptsource@ git+https://github.com/bigscience-workshop/promptsource@eval-hackathon->lm_eval==0.2.0) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.10/dist-packages (from altair>=3.2.0->streamlit==0.82->promptsource@ git+https://github.com/bigscience-workshop/promptsource@eval-hackathon->lm_eval==0.2.0) (1.40.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->lm_eval==0.2.0) (20.0.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate==0.4.0->lm_eval==0.2.0) (3.11.18)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate==0.4.0->lm_eval==0.2.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate==0.4.0->lm_eval==0.2.0) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate==0.4.0->lm_eval==0.2.0) (5.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate==0.4.0->lm_eval==0.2.0) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate==0.4.0->lm_eval==0.2.0) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate==0.4.0->lm_eval==0.2.0) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate==0.4.0->lm_eval==0.2.0) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate==0.4.0->lm_eval==0.2.0) (1.20.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit==0.82->promptsource@ git+https://github.com/bigscience-workshop/promptsource@eval-hackathon->lm_eval==0.2.0) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit==0.82->promptsource@ git+https://github.com/bigscience-workshop/promptsource@eval-hackathon->lm_eval==0.2.0) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit==0.82->promptsource@ git+https://github.com/bigscience-workshop/promptsource@eval-hackathon->lm_eval==0.2.0) (0.25.1)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl>=3.0.7->openai==0.13.0->lm_eval==0.2.0) (2.0.0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->promptsource@ git+https://github.com/bigscience-workshop/promptsource@eval-hackathon->lm_eval==0.2.0) (2025.2)\n",
            "Requirement already satisfied: types-pytz>=2022.1.1 in /usr/local/lib/python3.10/dist-packages (from pandas-stubs>=1.1.0.11->openai==0.13.0->lm_eval==0.2.0) (2025.2.0.20250516)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/lib/python3/dist-packages (from jinja2->promptsource@ git+https://github.com/bigscience-workshop/promptsource@eval-hackathon->lm_eval==0.2.0) (2.0.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.1->lm_eval==0.2.0) (1.15.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.1->lm_eval==0.2.0) (3.6.0)\n",
            "Requirement already satisfied: mccabe<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from flake8->promptsource@ git+https://github.com/bigscience-workshop/promptsource@eval-hackathon->lm_eval==0.2.0) (0.7.0)\n",
            "Requirement already satisfied: pycodestyle<2.14.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from flake8->promptsource@ git+https://github.com/bigscience-workshop/promptsource@eval-hackathon->lm_eval==0.2.0) (2.13.0)\n",
            "Requirement already satisfied: pyflakes<3.4.0,>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from flake8->promptsource@ git+https://github.com/bigscience-workshop/promptsource@eval-hackathon->lm_eval==0.2.0) (3.3.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython->streamlit==0.82->promptsource@ git+https://github.com/bigscience-workshop/promptsource@eval-hackathon->lm_eval==0.2.0) (4.0.12)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython->streamlit==0.82->promptsource@ git+https://github.com/bigscience-workshop/promptsource@eval-hackathon->lm_eval==0.2.0) (5.0.2)\n",
            "Requirement already satisfied: texttable in /usr/local/lib/python3.10/dist-packages (from py7zr->promptsource@ git+https://github.com/bigscience-workshop/promptsource@eval-hackathon->lm_eval==0.2.0) (1.7.0)\n",
            "Requirement already satisfied: pycryptodomex>=3.16.0 in /usr/local/lib/python3.10/dist-packages (from py7zr->promptsource@ git+https://github.com/bigscience-workshop/promptsource@eval-hackathon->lm_eval==0.2.0) (3.23.0)\n",
            "Requirement already satisfied: pyzstd>=0.15.9 in /usr/local/lib/python3.10/dist-packages (from py7zr->promptsource@ git+https://github.com/bigscience-workshop/promptsource@eval-hackathon->lm_eval==0.2.0) (0.17.0)\n",
            "Requirement already satisfied: pyppmd<1.2.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from py7zr->promptsource@ git+https://github.com/bigscience-workshop/promptsource@eval-hackathon->lm_eval==0.2.0) (1.1.1)\n",
            "Requirement already satisfied: pybcj<1.1.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from py7zr->promptsource@ git+https://github.com/bigscience-workshop/promptsource@eval-hackathon->lm_eval==0.2.0) (1.0.6)\n",
            "Requirement already satisfied: multivolumefile>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from py7zr->promptsource@ git+https://github.com/bigscience-workshop/promptsource@eval-hackathon->lm_eval==0.2.0) (0.2.3)\n",
            "Requirement already satisfied: inflate64<1.1.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from py7zr->promptsource@ git+https://github.com/bigscience-workshop/promptsource@eval-hackathon->lm_eval==0.2.0) (1.0.1)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from py7zr->promptsource@ git+https://github.com/bigscience-workshop/promptsource@eval-hackathon->lm_eval==0.2.0) (1.1.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest->lm_eval==0.2.0) (1.3.0)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest->lm_eval==0.2.0) (2.1.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.10/dist-packages (from pytest->lm_eval==0.2.0) (1.6.0)\n",
            "Installing collected packages: lm_eval\n",
            "\u001b[33m  DEPRECATION: Legacy editable install of lm_eval[colab]==0.2.0 from file:///content/evaluation-pipeline-2023 (setup.py develop) is deprecated. pip 25.3 will enforce this behaviour change. A possible replacement is to add a pyproject.toml or enable --use-pep517, and use setuptools >= 64. If the resulting installation is not behaving as expected, try using --config-settings editable_mode=compat. Please consult the setuptools documentation for more information. Discussion can be found at https://github.com/pypa/pip/issues/11457\u001b[0m\u001b[33m\n",
            "\u001b[0m  Running setup.py develop for lm_eval\n",
            "Successfully installed lm_eval-0.2.0\n",
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu113\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.13.0+cu113 (from versions: 1.11.0, 1.11.0+cu113, 1.12.0, 1.12.0+cu113, 1.12.1, 1.12.1+cu113, 1.13.0, 1.13.1, 2.0.0, 2.0.1, 2.1.0, 2.1.1, 2.1.2, 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.3.1, 2.4.0, 2.4.1, 2.5.0, 2.5.1, 2.6.0, 2.7.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torch==1.13.0+cu113\u001b[0m\u001b[31m\n",
            "\u001b[0mArchive:  filter_data.zip\n",
            "   creating: filter-data/\n",
            "   creating: filter-data/syllogisms_filtered/\n",
            "   creating: filter-data/syllogisms_filtered/syllogism_problems/\n",
            "  inflating: filter-data/syllogisms_filtered/syllogism_problems/syllogism_problems.json  \n",
            "  inflating: filter-data/syllogisms_filtered/syllogism_problems/LICENSE.txt  \n",
            "  inflating: filter-data/syllogisms_filtered/syllogisms_nonsense.train  \n",
            "   creating: filter-data/glue_filtered/\n",
            "  inflating: filter-data/glue_filtered/boolq.test.json  \n",
            "  inflating: filter-data/glue_filtered/boolq.train.json  \n",
            "  inflating: filter-data/glue_filtered/boolq.validation.json  \n",
            "  inflating: filter-data/glue_filtered/cola.test.json  \n",
            "  inflating: filter-data/glue_filtered/cola.train.json  \n",
            "  inflating: filter-data/glue_filtered/cola.validation.json  \n",
            "  inflating: filter-data/glue_filtered/mnli_mismatched.test.json  \n",
            "  inflating: filter-data/glue_filtered/mnli.test.json  \n",
            "  inflating: filter-data/glue_filtered/mnli.train.json  \n",
            "  inflating: filter-data/glue_filtered/mnli.validation_matched.json  \n",
            "  inflating: filter-data/glue_filtered/mnli.validation_mismatched.json  \n",
            "  inflating: filter-data/glue_filtered/mrpc.test.json  \n",
            "  inflating: filter-data/glue_filtered/mrpc.train.json  \n",
            "  inflating: filter-data/glue_filtered/mrpc.validation.json  \n",
            "  inflating: filter-data/glue_filtered/multirc.test.json  \n",
            "  inflating: filter-data/glue_filtered/multirc.train.json  \n",
            "  inflating: filter-data/glue_filtered/multirc.validation.json  \n",
            "  inflating: filter-data/glue_filtered/qnli.test.json  \n",
            "  inflating: filter-data/glue_filtered/qnli.train.json  \n",
            "  inflating: filter-data/glue_filtered/qnli.validation.json  \n",
            "  inflating: filter-data/glue_filtered/qqp.test.json  \n",
            "  inflating: filter-data/glue_filtered/qqp.train.json  \n",
            "  inflating: filter-data/glue_filtered/qqp.validation.json  \n",
            "  inflating: filter-data/glue_filtered/rte.test.json  \n",
            "  inflating: filter-data/glue_filtered/rte.train.json  \n",
            "  inflating: filter-data/glue_filtered/rte.validation.json  \n",
            "  inflating: filter-data/glue_filtered/sst2.test.json  \n",
            "  inflating: filter-data/glue_filtered/sst2.train.json  \n",
            "  inflating: filter-data/glue_filtered/sst2.validation.json  \n",
            "  inflating: filter-data/glue_filtered/wsc.test.json  \n",
            "  inflating: filter-data/glue_filtered/wsc.train.json  \n",
            "  inflating: filter-data/glue_filtered/wsc.validation.json  \n",
            "   creating: filter-data/blimp_filtered/\n",
            "  inflating: filter-data/blimp_filtered/island_effects.json  \n",
            "  inflating: filter-data/blimp_filtered/anaphor_agreement.json  \n",
            "  inflating: filter-data/blimp_filtered/argument_structure.json  \n",
            "  inflating: filter-data/blimp_filtered/determiner_noun_agreement.json  \n",
            "  inflating: filter-data/blimp_filtered/subject_verb_agreement.json  \n",
            "  inflating: filter-data/blimp_filtered/ellipsis.json  \n",
            "  inflating: filter-data/blimp_filtered/control_raising.json  \n",
            "  inflating: filter-data/blimp_filtered/quantifiers.json  \n",
            "  inflating: filter-data/blimp_filtered/irregular_forms.json  \n",
            "  inflating: filter-data/blimp_filtered/npi_licensing.json  \n",
            "  inflating: filter-data/blimp_filtered/binding.json  \n",
            "  inflating: filter-data/blimp_filtered/filler_gap.json  \n",
            "   creating: filter-data/msgs_filtered/\n",
            "  inflating: filter-data/msgs_filtered/syntactic_category_lexical_content_the.train.json  \n",
            "  inflating: filter-data/msgs_filtered/syntactic_category_control.validation.json  \n",
            "  inflating: filter-data/msgs_filtered/syntactic_category_control.train.json  \n",
            "  inflating: filter-data/msgs_filtered/main_verb_control.validation.json  \n",
            "  inflating: filter-data/msgs_filtered/lexical_content_the_control.validation.json  \n",
            "  inflating: filter-data/msgs_filtered/syntactic_category_relative_position.validation.json  \n",
            "  inflating: filter-data/msgs_filtered/control_raising_relative_token_position.validation.json  \n",
            "  inflating: filter-data/msgs_filtered/main_verb_lexical_content_the.validation.json  \n",
            "  inflating: filter-data/msgs_filtered/main_verb_relative_token_position.train.json  \n",
            "  inflating: filter-data/msgs_filtered/control_raising_control.train.json  \n",
            "  inflating: filter-data/msgs_filtered/syntactic_category_lexical_content_the.validation.json  \n",
            "  inflating: filter-data/msgs_filtered/control_raising_lexical_content_the.validation.json  \n",
            "  inflating: filter-data/msgs_filtered/lexical_content_the_control.train.json  \n",
            "  inflating: filter-data/msgs_filtered/control_raising_control.validation.json  \n",
            "  inflating: filter-data/msgs_filtered/relative_position_control.train.json  \n",
            "  inflating: filter-data/msgs_filtered/main_verb_control.train.json  \n",
            "  inflating: filter-data/msgs_filtered/control_raising_relative_token_position.train.json  \n",
            "  inflating: filter-data/msgs_filtered/control_raising_lexical_content_the.train.json  \n",
            "  inflating: filter-data/msgs_filtered/main_verb_lexical_content_the.train.json  \n",
            "  inflating: filter-data/msgs_filtered/syntactic_category_relative_position.train.json  \n",
            "  inflating: filter-data/msgs_filtered/relative_position_control.validation.json  \n",
            "  inflating: filter-data/msgs_filtered/main_verb_relative_token_position.validation.json  \n",
            "  inflating: filter-data/glue_filtered/mnli.test_matched.json  \n",
            "  inflating: filter-data/glue_filtered/mnli.test_mismatched.json  \n",
            "   creating: filter-data/supplement_filtered/\n",
            "  inflating: filter-data/supplement_filtered/qa_congruence_tricky.json  \n",
            "  inflating: filter-data/supplement_filtered/turn_taking.json  \n",
            "  inflating: filter-data/supplement_filtered/qa_congruence_easy.json  \n",
            "  inflating: filter-data/supplement_filtered/subject_aux_inversion.json  \n",
            "  inflating: filter-data/supplement_filtered/hypernym.json  \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "#@title Setup script { display-mode: \"form\" }\n",
        "#@markdown Run this cell to install the necessary packages (may take a few minutes).\n",
        "%%shell\n",
        "# Remove previous installation if it exists\n",
        "cd /content\n",
        "mkdir -p model_folder\n",
        "pip uninstall -y lm-eval\n",
        "rm -rf evaluation-pipeline-2023/\n",
        "\n",
        "# Install evaluation-pipeline\n",
        "git clone https://github.com/odenizddd/evaluation-pipeline-2023 &> /dev/null\n",
        "cd evaluation-pipeline-2023/\n",
        "python3.10 -m pip install -e \".[colab]\"\n",
        "# Install other necessary packages\n",
        "python3.10 -m pip install torch==1.13.0+cu113 torchvision==0.12.0+cu113 torchaudio==0.11.0 --extra-index-url https://download.pytorch.org/whl/cu113\n",
        "\n",
        "# Unpack dataset\n",
        "unzip filter_data.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iu0RtSOmFGq"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMHLEFyBM3k2",
        "outputId": "fa9c9e31-4b09-496e-8011-aaed072b9fff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy<2\n",
            "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m145.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.2.6\n",
            "    Uninstalling numpy-2.2.6:\n",
            "      Successfully uninstalled numpy-2.2.6\n",
            "Successfully installed numpy-1.26.4\n"
          ]
        }
      ],
      "source": [
        "!python3.10 -m pip install 'numpy<2'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4ftRocjmIa4",
        "outputId": "021eb315-7a69-4d45-a242-011adf1f6970"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/evaluation-pipeline-2023\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Some weights of the model checkpoint at /content/elc-bert-replica were not used when initializing LtgBertForMaskedLM: ['grad_scaler', 'epoch', 'optimizer', 'model', 'args', 'global_step', 'scheduler']\n",
            "- This IS expected if you are initializing LtgBertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing LtgBertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of LtgBertForMaskedLM were not initialized from the model checkpoint at /content/elc-bert-replica and are newly initialized: ['transformer.layers.8.attention.post_layer_norm.weight', 'transformer.layers.8.mlp.mlp.4.weight', 'transformer.layers.11.attention.out_proj.bias', 'transformer.layers.9.mlp.mlp.1.weight', 'transformer.layers.4.attention.in_proj_v.bias', 'transformer.layers.2.attention.out_proj.bias', 'transformer.layers.0.attention.post_layer_norm.weight', 'transformer.layers.4.attention.in_proj_qk.weight', 'transformer.layers.2.attention.in_proj_v.weight', 'transformer.layers.9.attention.post_layer_norm.weight', 'transformer.layers.8.attention.out_proj.weight', 'transformer.layers.10.attention.in_proj_v.weight', 'transformer.layers.5.attention.in_proj_qk.bias', 'transformer.layers.7.attention.position_indices', 'transformer.layers.0.attention.out_proj.weight', 'transformer.layers.8.attention.position_indices', 'transformer.layers.4.mlp.mlp.4.weight', 'transformer.layers.4.attention.post_layer_norm.bias', 'embedding.relative_embedding', 'transformer.layers.1.prev_layer_weights', 'transformer.layers.1.attention.in_proj_qk.bias', 'transformer.layers.3.mlp.mlp.1.weight', 'embedding.relative_layer_norm.bias', 'transformer.layers.7.attention.post_layer_norm.weight', 'transformer.layers.9.attention.in_proj_v.bias', 'transformer.layers.7.mlp.mlp.4.weight', 'transformer.layers.3.attention.post_layer_norm.weight', 'transformer.layers.3.attention.in_proj_v.weight', 'transformer.layers.5.attention.out_proj.weight', 'transformer.layers.2.mlp.mlp.4.weight', 'transformer.layers.8.attention.in_proj_qk.weight', 'transformer.layers.0.mlp.mlp.4.weight', 'transformer.layers.0.attention.in_proj_v.bias', 'transformer.layers.9.attention.position_indices', 'transformer.layers.11.mlp.mlp.4.weight', 'transformer.layers.7.attention.post_layer_norm.bias', 'transformer.layers.5.attention.post_layer_norm.weight', 'transformer.layers.4.attention.in_proj_qk.bias', 'transformer.layers.5.attention.in_proj_v.weight', 'transformer.layers.6.attention.post_layer_norm.bias', 'transformer.layers.3.attention.in_proj_qk.weight', 'transformer.layers.7.mlp.mlp.1.weight', 'transformer.layers.10.attention.out_proj.weight', 'transformer.layers.4.mlp.mlp.1.weight', 'transformer.layers.6.attention.out_proj.bias', 'transformer.layers.3.prev_layer_weights', 'transformer.layers.3.attention.out_proj.weight', 'transformer.layers.4.attention.in_proj_v.weight', 'classifier.nonlinearity.1.bias', 'transformer.layers.0.attention.position_indices', 'transformer.layers.9.mlp.mlp.4.weight', 'transformer.layers.11.attention.in_proj_v.bias', 'classifier.nonlinearity.1.weight', 'transformer.layers.3.mlp.mlp.4.weight', 'classifier.nonlinearity.5.bias', 'transformer.layers.6.attention.in_proj_qk.weight', 'transformer.layers.8.mlp.mlp.1.weight', 'transformer.layers.0.prev_layer_weights', 'transformer.layers.10.mlp.mlp.4.weight', 'transformer.layers.10.mlp.mlp.1.weight', 'transformer.layers.6.mlp.mlp.1.weight', 'transformer.layers.6.attention.in_proj_v.bias', 'transformer.layers.2.prev_layer_weights', 'transformer.layers.2.attention.post_layer_norm.bias', 'transformer.layers.4.prev_layer_weights', 'transformer.layers.5.attention.position_indices', 'transformer.layers.3.attention.position_indices', 'transformer.layers.0.attention.post_layer_norm.bias', 'transformer.layers.5.attention.out_proj.bias', 'transformer.layers.8.attention.out_proj.bias', 'transformer.layers.11.attention.in_proj_v.weight', 'transformer.layers.10.prev_layer_weights', 'transformer.layers.5.mlp.mlp.1.weight', 'transformer.layers.7.prev_layer_weights', 'transformer.layers.5.mlp.mlp.4.weight', 'transformer.layers.7.attention.in_proj_qk.weight', 'transformer.layers.4.attention.position_indices', 'transformer.layers.3.attention.out_proj.bias', 'transformer.layers.1.attention.position_indices', 'transformer.layers.6.attention.post_layer_norm.weight', 'transformer.layers.10.attention.post_layer_norm.bias', 'transformer.layers.10.attention.in_proj_v.bias', 'transformer.layers.8.attention.in_proj_v.weight', 'transformer.layers.11.prev_layer_weights', 'transformer.layers.2.attention.in_proj_qk.bias', 'transformer.layers.6.attention.in_proj_qk.bias', 'transformer.layers.8.attention.in_proj_qk.bias', 'transformer.layers.9.prev_layer_weights', 'transformer.layers.9.attention.in_proj_qk.bias', 'transformer.layers.1.mlp.mlp.1.weight', 'transformer.layers.1.attention.out_proj.bias', 'transformer.layers.3.attention.in_proj_qk.bias', 'transformer.layers.11.attention.in_proj_qk.bias', 'transformer.layers.6.prev_layer_weights', 'transformer.layers.1.attention.in_proj_qk.weight', 'transformer.layers.2.attention.position_indices', 'transformer.layers.6.attention.position_indices', 'transformer.layers.10.attention.out_proj.bias', 'transformer.layers.2.attention.in_proj_qk.weight', 'transformer.layers.1.attention.in_proj_v.bias', 'transformer.layers.7.attention.out_proj.weight', 'transformer.layers.5.prev_layer_weights', 'transformer.layers.6.attention.in_proj_v.weight', 'transformer.layers.0.attention.in_proj_qk.weight', 'transformer.layers.1.attention.in_proj_v.weight', 'transformer.layers.8.attention.in_proj_v.bias', 'transformer.layers.3.attention.in_proj_v.bias', 'transformer.layers.7.attention.in_proj_v.bias', 'transformer.layers.6.attention.out_proj.weight', 'transformer.layers.0.attention.out_proj.bias', 'transformer.layers.11.attention.out_proj.weight', 'transformer.layers.7.attention.in_proj_v.weight', 'transformer.layers.4.attention.out_proj.bias', 'transformer.layers.1.attention.post_layer_norm.bias', 'transformer.layers.3.attention.post_layer_norm.bias', 'transformer.layers.4.attention.out_proj.weight', 'transformer.layers.11.mlp.mlp.1.weight', 'transformer.layers.9.attention.out_proj.weight', 'transformer.layers.10.attention.post_layer_norm.weight', 'transformer.layers.10.attention.in_proj_qk.weight', 'transformer.layers.5.attention.in_proj_qk.weight', 'transformer.layers.4.attention.post_layer_norm.weight', 'transformer.layers.1.attention.post_layer_norm.weight', 'transformer.layers.0.attention.in_proj_qk.bias', 'transformer.layers.2.attention.in_proj_v.bias', 'transformer.layers.2.attention.post_layer_norm.weight', 'transformer.layers.2.mlp.mlp.1.weight', 'transformer.layers.6.mlp.mlp.4.weight', 'transformer.layers.7.attention.out_proj.bias', 'transformer.layers.8.prev_layer_weights', 'embedding.word_embedding.weight', 'transformer.layers.2.attention.out_proj.weight', 'transformer.layers.1.attention.out_proj.weight', 'transformer.layers.11.attention.post_layer_norm.bias', 'transformer.layers.11.attention.in_proj_qk.weight', 'transformer.layers.5.attention.post_layer_norm.bias', 'transformer.layers.5.attention.in_proj_v.bias', 'transformer.layers.9.attention.in_proj_v.weight', 'transformer.layers.1.mlp.mlp.4.weight', 'classifier.nonlinearity.5.weight', 'transformer.layers.8.attention.post_layer_norm.bias', 'transformer.layers.11.attention.post_layer_norm.weight', 'transformer.layers.0.attention.in_proj_v.weight', 'transformer.layers.0.mlp.mlp.1.weight', 'transformer.layers.10.attention.position_indices', 'embedding.relative_layer_norm.weight', 'transformer.layers.9.attention.out_proj.bias', 'transformer.layers.10.attention.in_proj_qk.bias', 'transformer.layers.9.attention.post_layer_norm.bias', 'transformer.layers.11.attention.position_indices', 'transformer.layers.7.attention.in_proj_qk.bias', 'transformer.layers.9.attention.in_proj_qk.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running aoa...\n",
            "\n",
            "» Evaluating model on predicting the age of acquisition of words.\n",
            "\n",
            "» Collecting model average surprisal values.\n",
            "100% 1747/1747 [04:33<00:00,  6.39it/s]\n",
            "\n",
            "» Fitting regression models using leave-one-out cross-validation.\n",
            "Finished aoa.\n"
          ]
        }
      ],
      "source": [
        "#@title Load model and evaluate (BLiMP) { display-mode: \"form\" }\n",
        "model = \"/content/elc-bert-replica\" #@param {\"type\": \"string\"}\n",
        "model_type = \"encoder\" #@param [\"decoder\", \"encoder\", \"encoder-decoder\"]\n",
        "# file_name = \"examples3.csv\" #@param {\"type\": \"string\"}\n",
        "# model_names = [\"opt-125m\", \"opt-350m\", \"opt-1.3b\", \"opt-2.7b\"] #@param {\"type\": \"raw\"}\n",
        "\n",
        "%cd /content/evaluation-pipeline-2023\n",
        "!python3.10 /content/evaluation-pipeline-2023/babylm_eval.py \\\n",
        "  \"$model\" \\\n",
        "  \"$model_type\" \\\n",
        "  -t \"blimp\" \\\n",
        "  --trust_remote_code \\\n",
        "  --run_aoa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKzgjovKEDTh",
        "outputId": "ba0da0be-54f2-408b-f387-bbe0b71d3e98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/evaluation-pipeline-2023/./collect_results.py\", line 71, in <module>\n",
            "    task_dicts[task] = make_task_dict(task, preds_path)\n",
            "  File \"/content/evaluation-pipeline-2023/./collect_results.py\", line 38, in make_task_dict\n",
            "    raise FileNotFoundError(f\"Warning: no predictions found for the \\\"{task_name}\\\" ({task_type}) task!\")\n",
            "FileNotFoundError: Warning: no predictions found for the \"cola\" (glue) task!\n"
          ]
        }
      ],
      "source": [
        "!python3.10 ./collect_results.py /content/evaluation-pipeline-2023/elc-bert-replica"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install evaluate\n"
      ],
      "metadata": {
        "id": "i64lw-sQH3I9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f812194c-48f6-4f5c-cfef-243ba8ddf402"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.14.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.0.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.31.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.15)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
            "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: evaluate\n",
            "Successfully installed evaluate-0.4.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/elc-bert-replica/finetune"
      ],
      "metadata": {
        "id": "okAGI0YB0Fe0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PorzVcsqTrbf",
        "outputId": "f5288ab0-a0aa-459f-bc4b-0927680ce395"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/evaluation-pipeline-2023\n",
            "2025-05-21 17:19:36.040284: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1747847976.297918    3648 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1747847976.363282    3648 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-21 17:19:36.883855: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/evaluation-pipeline-2023/finetune_classification.py\", line 731, in <module>\n",
            "    main()\n",
            "  File \"/content/evaluation-pipeline-2023/finetune_classification.py\", line 241, in main\n",
            "    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
            "                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/hf_argparser.py\", line 367, in parse_args_into_dataclasses\n",
            "    raise ValueError(f\"Some specified arguments are not used by the HfArgumentParser: {remaining_args}\")\n",
            "ValueError: Some specified arguments are not used by the HfArgumentParser: ['--evaluation_strategy', 'steps']\n",
            "2025-05-21 17:19:54.086330: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1747847994.106694    3777 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1747847994.112940    3777 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-21 17:19:54.132263: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/evaluation-pipeline-2023/finetune_classification.py\", line 731, in <module>\n",
            "    main()\n",
            "  File \"/content/evaluation-pipeline-2023/finetune_classification.py\", line 241, in main\n",
            "    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
            "                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/hf_argparser.py\", line 367, in parse_args_into_dataclasses\n",
            "    raise ValueError(f\"Some specified arguments are not used by the HfArgumentParser: {remaining_args}\")\n",
            "ValueError: Some specified arguments are not used by the HfArgumentParser: ['--evaluation_strategy', 'steps']\n",
            "2025-05-21 17:20:05.045866: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1747848005.066622    3838 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1747848005.072779    3838 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-21 17:20:05.092844: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/evaluation-pipeline-2023/finetune_classification.py\", line 731, in <module>\n",
            "    main()\n",
            "  File \"/content/evaluation-pipeline-2023/finetune_classification.py\", line 241, in main\n",
            "    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
            "                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/hf_argparser.py\", line 367, in parse_args_into_dataclasses\n",
            "    raise ValueError(f\"Some specified arguments are not used by the HfArgumentParser: {remaining_args}\")\n",
            "ValueError: Some specified arguments are not used by the HfArgumentParser: ['--evaluation_strategy', 'steps']\n",
            "2025-05-21 17:20:16.107683: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1747848016.127869    3897 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1747848016.133845    3897 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-21 17:20:16.154289: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/evaluation-pipeline-2023/finetune_classification.py\", line 731, in <module>\n",
            "    main()\n",
            "  File \"/content/evaluation-pipeline-2023/finetune_classification.py\", line 241, in main\n",
            "    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
            "                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/hf_argparser.py\", line 367, in parse_args_into_dataclasses\n",
            "    raise ValueError(f\"Some specified arguments are not used by the HfArgumentParser: {remaining_args}\")\n",
            "ValueError: Some specified arguments are not used by the HfArgumentParser: ['--evaluation_strategy', 'steps']\n",
            "2025-05-21 17:20:27.108609: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1747848027.128981    3956 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1747848027.135037    3956 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-21 17:20:27.155244: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/evaluation-pipeline-2023/finetune_classification.py\", line 731, in <module>\n",
            "    main()\n",
            "  File \"/content/evaluation-pipeline-2023/finetune_classification.py\", line 241, in main\n",
            "    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
            "                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/hf_argparser.py\", line 367, in parse_args_into_dataclasses\n",
            "    raise ValueError(f\"Some specified arguments are not used by the HfArgumentParser: {remaining_args}\")\n",
            "ValueError: Some specified arguments are not used by the HfArgumentParser: ['--evaluation_strategy', 'steps']\n",
            "2025-05-21 17:20:37.941427: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1747848037.961532    4017 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1747848037.967616    4017 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-21 17:20:37.988299: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/evaluation-pipeline-2023/finetune_classification.py\", line 731, in <module>\n",
            "    main()\n",
            "  File \"/content/evaluation-pipeline-2023/finetune_classification.py\", line 241, in main\n",
            "    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
            "                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/hf_argparser.py\", line 367, in parse_args_into_dataclasses\n",
            "    raise ValueError(f\"Some specified arguments are not used by the HfArgumentParser: {remaining_args}\")\n",
            "ValueError: Some specified arguments are not used by the HfArgumentParser: ['--evaluation_strategy', 'steps']\n",
            "2025-05-21 17:20:49.972048: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1747848049.992060    4080 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1747848049.998190    4080 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-21 17:20:50.017742: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/evaluation-pipeline-2023/finetune_classification.py\", line 731, in <module>\n",
            "    main()\n",
            "  File \"/content/evaluation-pipeline-2023/finetune_classification.py\", line 241, in main\n",
            "    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
            "                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/hf_argparser.py\", line 367, in parse_args_into_dataclasses\n",
            "    raise ValueError(f\"Some specified arguments are not used by the HfArgumentParser: {remaining_args}\")\n",
            "ValueError: Some specified arguments are not used by the HfArgumentParser: ['--evaluation_strategy', 'steps']\n",
            "2025-05-21 17:21:01.039219: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1747848061.059428    4135 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1747848061.065594    4135 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-21 17:21:01.084937: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/evaluation-pipeline-2023/finetune_classification.py\", line 731, in <module>\n",
            "    main()\n",
            "  File \"/content/evaluation-pipeline-2023/finetune_classification.py\", line 241, in main\n",
            "    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
            "                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/hf_argparser.py\", line 367, in parse_args_into_dataclasses\n",
            "    raise ValueError(f\"Some specified arguments are not used by the HfArgumentParser: {remaining_args}\")\n",
            "ValueError: Some specified arguments are not used by the HfArgumentParser: ['--evaluation_strategy', 'steps']\n",
            "2025-05-21 17:21:12.009578: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1747848072.029518    4198 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1747848072.035607    4198 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-21 17:21:12.055813: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/evaluation-pipeline-2023/finetune_classification.py\", line 731, in <module>\n",
            "    main()\n",
            "  File \"/content/evaluation-pipeline-2023/finetune_classification.py\", line 241, in main\n",
            "    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
            "                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/hf_argparser.py\", line 367, in parse_args_into_dataclasses\n",
            "    raise ValueError(f\"Some specified arguments are not used by the HfArgumentParser: {remaining_args}\")\n",
            "ValueError: Some specified arguments are not used by the HfArgumentParser: ['--evaluation_strategy', 'steps']\n",
            "2025-05-21 17:21:22.865754: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1747848082.900574    4257 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1747848082.911421    4257 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-21 17:21:22.944576: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/evaluation-pipeline-2023/finetune_classification.py\", line 731, in <module>\n",
            "    main()\n",
            "  File \"/content/evaluation-pipeline-2023/finetune_classification.py\", line 241, in main\n",
            "    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
            "                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/hf_argparser.py\", line 367, in parse_args_into_dataclasses\n",
            "    raise ValueError(f\"Some specified arguments are not used by the HfArgumentParser: {remaining_args}\")\n",
            "ValueError: Some specified arguments are not used by the HfArgumentParser: ['--evaluation_strategy', 'steps']\n",
            "2025-05-21 17:21:33.631712: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1747848093.663916    4318 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1747848093.673759    4318 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-21 17:21:33.704175: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/evaluation-pipeline-2023/finetune_classification.py\", line 731, in <module>\n",
            "    main()\n",
            "  File \"/content/evaluation-pipeline-2023/finetune_classification.py\", line 241, in main\n",
            "    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
            "                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/hf_argparser.py\", line 367, in parse_args_into_dataclasses\n",
            "    raise ValueError(f\"Some specified arguments are not used by the HfArgumentParser: {remaining_args}\")\n",
            "ValueError: Some specified arguments are not used by the HfArgumentParser: ['--evaluation_strategy', 'steps']\n",
            "2025-05-21 17:21:44.940121: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1747848104.960099    4382 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1747848104.966573    4382 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-21 17:21:44.986275: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/evaluation-pipeline-2023/finetune_classification.py\", line 731, in <module>\n",
            "    main()\n",
            "  File \"/content/evaluation-pipeline-2023/finetune_classification.py\", line 241, in main\n",
            "    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
            "                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/hf_argparser.py\", line 367, in parse_args_into_dataclasses\n",
            "    raise ValueError(f\"Some specified arguments are not used by the HfArgumentParser: {remaining_args}\")\n",
            "ValueError: Some specified arguments are not used by the HfArgumentParser: ['--evaluation_strategy', 'steps']\n",
            "2025-05-21 17:21:56.054520: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1747848116.073948    4441 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1747848116.079798    4441 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-21 17:21:56.099201: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/evaluation-pipeline-2023/finetune_classification.py\", line 731, in <module>\n",
            "    main()\n",
            "  File \"/content/evaluation-pipeline-2023/finetune_classification.py\", line 241, in main\n",
            "    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
            "                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/hf_argparser.py\", line 367, in parse_args_into_dataclasses\n",
            "    raise ValueError(f\"Some specified arguments are not used by the HfArgumentParser: {remaining_args}\")\n",
            "ValueError: Some specified arguments are not used by the HfArgumentParser: ['--evaluation_strategy', 'steps']\n",
            "2025-05-21 17:22:07.114170: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1747848127.133963    4502 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1747848127.140021    4502 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-21 17:22:07.159880: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/evaluation-pipeline-2023/finetune_classification.py\", line 731, in <module>\n",
            "    main()\n",
            "  File \"/content/evaluation-pipeline-2023/finetune_classification.py\", line 241, in main\n",
            "    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
            "                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/hf_argparser.py\", line 367, in parse_args_into_dataclasses\n",
            "    raise ValueError(f\"Some specified arguments are not used by the HfArgumentParser: {remaining_args}\")\n",
            "ValueError: Some specified arguments are not used by the HfArgumentParser: ['--evaluation_strategy', 'steps']\n",
            "2025-05-21 17:22:18.180796: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1747848138.201025    4561 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1747848138.207328    4561 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-21 17:22:18.227931: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/evaluation-pipeline-2023/finetune_classification.py\", line 731, in <module>\n",
            "    main()\n",
            "  File \"/content/evaluation-pipeline-2023/finetune_classification.py\", line 241, in main\n",
            "    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
            "                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/hf_argparser.py\", line 367, in parse_args_into_dataclasses\n",
            "    raise ValueError(f\"Some specified arguments are not used by the HfArgumentParser: {remaining_args}\")\n",
            "ValueError: Some specified arguments are not used by the HfArgumentParser: ['--evaluation_strategy', 'steps']\n",
            "2025-05-21 17:22:29.093527: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1747848149.115096    4620 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1747848149.121327    4620 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-21 17:22:29.141927: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/evaluation-pipeline-2023/finetune_classification.py\", line 731, in <module>\n",
            "    main()\n",
            "  File \"/content/evaluation-pipeline-2023/finetune_classification.py\", line 241, in main\n",
            "    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
            "                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/hf_argparser.py\", line 367, in parse_args_into_dataclasses\n",
            "    raise ValueError(f\"Some specified arguments are not used by the HfArgumentParser: {remaining_args}\")\n",
            "ValueError: Some specified arguments are not used by the HfArgumentParser: ['--evaluation_strategy', 'steps']\n",
            "2025-05-21 17:22:40.023545: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1747848160.044956    4677 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1747848160.051023    4677 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-21 17:22:40.071303: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/evaluation-pipeline-2023/finetune_classification.py\", line 731, in <module>\n",
            "    main()\n",
            "  File \"/content/evaluation-pipeline-2023/finetune_classification.py\", line 241, in main\n",
            "    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
            "                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/hf_argparser.py\", line 367, in parse_args_into_dataclasses\n",
            "    raise ValueError(f\"Some specified arguments are not used by the HfArgumentParser: {remaining_args}\")\n",
            "ValueError: Some specified arguments are not used by the HfArgumentParser: ['--evaluation_strategy', 'steps']\n",
            "2025-05-21 17:22:51.108887: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1747848171.129546    4736 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1747848171.135496    4736 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-21 17:22:51.155423: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/evaluation-pipeline-2023/finetune_classification.py\", line 731, in <module>\n",
            "    main()\n",
            "  File \"/content/evaluation-pipeline-2023/finetune_classification.py\", line 241, in main\n",
            "    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
            "                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/hf_argparser.py\", line 367, in parse_args_into_dataclasses\n",
            "    raise ValueError(f\"Some specified arguments are not used by the HfArgumentParser: {remaining_args}\")\n",
            "ValueError: Some specified arguments are not used by the HfArgumentParser: ['--evaluation_strategy', 'steps']\n",
            "2025-05-21 17:23:02.215821: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1747848182.236093    4795 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1747848182.242274    4795 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-21 17:23:02.262827: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/evaluation-pipeline-2023/finetune_classification.py\", line 731, in <module>\n",
            "    main()\n",
            "  File \"/content/evaluation-pipeline-2023/finetune_classification.py\", line 241, in main\n",
            "    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
            "                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/hf_argparser.py\", line 367, in parse_args_into_dataclasses\n",
            "    raise ValueError(f\"Some specified arguments are not used by the HfArgumentParser: {remaining_args}\")\n",
            "ValueError: Some specified arguments are not used by the HfArgumentParser: ['--evaluation_strategy', 'steps']\n",
            "2025-05-21 17:23:13.135116: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1747848193.168399    4856 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1747848193.178280    4856 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-21 17:23:13.210394: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/evaluation-pipeline-2023/finetune_classification.py\", line 731, in <module>\n",
            "    main()\n",
            "  File \"/content/evaluation-pipeline-2023/finetune_classification.py\", line 241, in main\n",
            "    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
            "                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/hf_argparser.py\", line 367, in parse_args_into_dataclasses\n",
            "    raise ValueError(f\"Some specified arguments are not used by the HfArgumentParser: {remaining_args}\")\n",
            "ValueError: Some specified arguments are not used by the HfArgumentParser: ['--evaluation_strategy', 'steps']\n",
            "2025-05-21 17:23:23.685717: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1747848203.719073    4915 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1747848203.727230    4915 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-21 17:23:23.757860: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/evaluation-pipeline-2023/finetune_classification.py\", line 731, in <module>\n",
            "    main()\n",
            "  File \"/content/evaluation-pipeline-2023/finetune_classification.py\", line 241, in main\n",
            "    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
            "                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/hf_argparser.py\", line 367, in parse_args_into_dataclasses\n",
            "    raise ValueError(f\"Some specified arguments are not used by the HfArgumentParser: {remaining_args}\")\n",
            "ValueError: Some specified arguments are not used by the HfArgumentParser: ['--evaluation_strategy', 'steps']\n",
            "2025-05-21 17:23:34.649200: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1747848214.668416    4976 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1747848214.674189    4976 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-21 17:23:34.693387: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/evaluation-pipeline-2023/finetune_classification.py\", line 731, in <module>\n",
            "    main()\n",
            "  File \"/content/evaluation-pipeline-2023/finetune_classification.py\", line 241, in main\n",
            "    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
            "                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/hf_argparser.py\", line 367, in parse_args_into_dataclasses\n",
            "    raise ValueError(f\"Some specified arguments are not used by the HfArgumentParser: {remaining_args}\")\n",
            "ValueError: Some specified arguments are not used by the HfArgumentParser: ['--evaluation_strategy', 'steps']\n"
          ]
        }
      ],
      "source": [
        "#@title Load model and evaluate ((Super)GLUE) { display-mode: \"form\" }\n",
        "#@markdown Run this cell to fine-tune your model on (Super)GLUE tasks.\n",
        "#@markdown We provide some default hyperparameters that you may adjust.\n",
        "model = \"/content/elc-bert-replica\" #@param {\"type\": \"string\"}\n",
        "learning_rate = 5e-5 #@param {\"type\": \"number\"}\n",
        "batch_size = 64 #@param {\"type\": \"integer\"}\n",
        "eval_every = 200 #@param {\"type\": \"integer\"}\n",
        "patience = 10 #@param {\"type\": \"integer\"}\n",
        "max_epochs = 10 #@param {\"type\": \"integer\"}\n",
        "seed = 12 #@param {\"type\": \"integer\"}\n",
        "# file_name = \"examples3.csv\" #@param {\"type\": \"string\"}\n",
        "# model_names = [\"opt-125m\", \"opt-350m\", \"opt-1.3b\", \"opt-2.7b\"] #@param {\"type\": \"raw\"}\n",
        "\n",
        "%cd /content/evaluation-pipeline-2023\n",
        "!./finetune_all_tasks.sh \\\n",
        "    \"$model\" \\\n",
        "    \"$learning_rate\" \\\n",
        "    \"$patience\" \\\n",
        "    \"$batch_size\" \\\n",
        "    \"$eval_every\" \\\n",
        "    \"$max_epochs\" \\\n",
        "    \"$seed\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTQqPW7TiYJS",
        "outputId": "c5f6f1a0-37c9-47b9-ef04-49751ea08e2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/evaluation-pipeline-2023\n"
          ]
        }
      ],
      "source": [
        "!cd /content\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "# Load the tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "save_directory = \"./bert_model2\"\n",
        "\n",
        "tokenizer.save_pretrained(save_directory)\n",
        "model.save_pretrained(save_directory, safe_serialization=False)\n",
        "!pwd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PAf8AQi8IwPi",
        "outputId": "706a5a87-f48c-46e6-9013-8255d4515b43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.0)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.2.5)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.2.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.31.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (25.0)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.18.0)\n",
            "Requirement already satisfied: urllib3>=1.25.10 in /usr/local/lib/python3.10/dist-packages (from responses<0.19->evaluate) (2.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2025.4.26)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (20.0.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.11.18)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (5.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (4.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install evaluate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "swkYEN28FaJl",
        "outputId": "1186b1a6-8acd-46ce-f102-f994d71ec462"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/zeroshot.zip'"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import shutil\n",
        "\n",
        "# Full paths to the folders you want to zip\n",
        "shutil.make_archive('/content/finetune', 'zip', '/content/evaluation-pipeline-2023/elc-bert-replica/finetune')\n",
        "shutil.make_archive('/content/zeroshot', 'zip', '/content/evaluation-pipeline-2023/elc-bert-replica/zeroshot')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# Full paths to the folders you want to zip\n",
        "shutil.make_archive('/content/evaluation-pipeline-2023', 'zip', '/content/evaluation-pipeline-2023')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "WzzqToZ8iGOO",
        "outputId": "113006e9-da86-4dc7-ea54-257a347ed58b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/evaluation-pipeline-2023.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "cd /content/evaluation-pipeline-2023\n",
        "ls -l"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "auwytR0foP-e",
        "outputId": "c186f945-b633-4268-be2a-3d8b3328866d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 81160\n",
            "drwxr-xr-x 2 root root     4096 May 11 18:28 aoa_data\n",
            "drwxr-xr-x 2 root root     4096 May 11 18:28 assets\n",
            "-rw-r--r-- 1 root root     4804 May 11 18:28 babylm_eval.py\n",
            "-rw-r--r-- 1 root root       34 May 11 18:28 CODEOWNERS\n",
            "-rw-r--r-- 1 root root     3713 May 11 18:28 collect_results.py\n",
            "drwxr-xr-x 3 root root     4096 May 11 18:28 docs\n",
            "drwxr-xr-x 7 root root     4096 May 11 18:28 filter-data\n",
            "-rw-r--r-- 1 root root 61762326 May 11 18:28 filter_data.zip\n",
            "-rwxrwxrwx 1 root root     1003 May 11 18:28 finetune_all_tasks.sh\n",
            "-rw-r--r-- 1 root root    32453 May 11 18:46 finetune_classification.py\n",
            "-rwxrwxrwx 1 root root     1255 May 11 18:28 finetune_model.sh\n",
            "-rw-r--r-- 1 root root       40 May 11 18:28 ignore.txt\n",
            "-rw-r--r-- 1 root root     1067 May 11 18:28 LICENSE.md\n",
            "drwxr-xr-x 7 root root     4096 May 11 18:28 lm_eval\n",
            "drwxr-xr-x 2 root root     4096 May 11 18:29 lm_eval.egg-info\n",
            "-rw-r--r-- 1 root root     7347 May 11 18:28 main.py\n",
            "-rw-r--r-- 1 root root    13536 May 11 18:28 README.md\n",
            "-rw-r--r-- 1 root root 21205894 May 11 18:28 sample_predictions.json\n",
            "drwxr-xr-x 2 root root     4096 May 11 18:28 scripts\n",
            "-rw-r--r-- 1 root root     2033 May 11 18:28 setup.py\n",
            "drwxr-xr-x 2 root root     4096 May 11 18:28 templates\n",
            "drwxr-xr-x 3 root root     4096 May 11 18:28 tests\n",
            "drwxr-xr-x 2 root root     4096 May 11 18:28 transformers_modified\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod 777 ./*.sh"
      ],
      "metadata": {
        "id": "KhzRoOdmpGVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"/content/elc-bert-replica\", weights_only=False)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_W73nRbLzD1j",
        "outputId": "fbeb746e-dea1-4edb-f0d0-15bd74d95c0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The repository for /content/elc-bert-replica contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co//content/elc-bert-replica.\n",
            "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
            "\n",
            "Do you wish to run the custom code? [y/N] y\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of LtgBertForSequenceClassification were not initialized from the model checkpoint at /content/elc-bert-replica and are newly initialized: ['embedding.relative_embedding', 'embedding.relative_layer_norm.bias', 'embedding.relative_layer_norm.weight', 'embedding.word_embedding.weight', 'transformer.layers.0.attention.in_proj_qk.bias', 'transformer.layers.0.attention.in_proj_qk.weight', 'transformer.layers.0.attention.in_proj_v.bias', 'transformer.layers.0.attention.in_proj_v.weight', 'transformer.layers.0.attention.out_proj.bias', 'transformer.layers.0.attention.out_proj.weight', 'transformer.layers.0.attention.position_indices', 'transformer.layers.0.attention.post_layer_norm.bias', 'transformer.layers.0.attention.post_layer_norm.weight', 'transformer.layers.0.mlp.mlp.1.weight', 'transformer.layers.0.mlp.mlp.4.weight', 'transformer.layers.0.prev_layer_weights', 'transformer.layers.1.attention.in_proj_qk.bias', 'transformer.layers.1.attention.in_proj_qk.weight', 'transformer.layers.1.attention.in_proj_v.bias', 'transformer.layers.1.attention.in_proj_v.weight', 'transformer.layers.1.attention.out_proj.bias', 'transformer.layers.1.attention.out_proj.weight', 'transformer.layers.1.attention.position_indices', 'transformer.layers.1.attention.post_layer_norm.bias', 'transformer.layers.1.attention.post_layer_norm.weight', 'transformer.layers.1.mlp.mlp.1.weight', 'transformer.layers.1.mlp.mlp.4.weight', 'transformer.layers.1.prev_layer_weights', 'transformer.layers.10.attention.in_proj_qk.bias', 'transformer.layers.10.attention.in_proj_qk.weight', 'transformer.layers.10.attention.in_proj_v.bias', 'transformer.layers.10.attention.in_proj_v.weight', 'transformer.layers.10.attention.out_proj.bias', 'transformer.layers.10.attention.out_proj.weight', 'transformer.layers.10.attention.position_indices', 'transformer.layers.10.attention.post_layer_norm.bias', 'transformer.layers.10.attention.post_layer_norm.weight', 'transformer.layers.10.mlp.mlp.1.weight', 'transformer.layers.10.mlp.mlp.4.weight', 'transformer.layers.10.prev_layer_weights', 'transformer.layers.11.attention.in_proj_qk.bias', 'transformer.layers.11.attention.in_proj_qk.weight', 'transformer.layers.11.attention.in_proj_v.bias', 'transformer.layers.11.attention.in_proj_v.weight', 'transformer.layers.11.attention.out_proj.bias', 'transformer.layers.11.attention.out_proj.weight', 'transformer.layers.11.attention.position_indices', 'transformer.layers.11.attention.post_layer_norm.bias', 'transformer.layers.11.attention.post_layer_norm.weight', 'transformer.layers.11.mlp.mlp.1.weight', 'transformer.layers.11.mlp.mlp.4.weight', 'transformer.layers.11.prev_layer_weights', 'transformer.layers.2.attention.in_proj_qk.bias', 'transformer.layers.2.attention.in_proj_qk.weight', 'transformer.layers.2.attention.in_proj_v.bias', 'transformer.layers.2.attention.in_proj_v.weight', 'transformer.layers.2.attention.out_proj.bias', 'transformer.layers.2.attention.out_proj.weight', 'transformer.layers.2.attention.position_indices', 'transformer.layers.2.attention.post_layer_norm.bias', 'transformer.layers.2.attention.post_layer_norm.weight', 'transformer.layers.2.mlp.mlp.1.weight', 'transformer.layers.2.mlp.mlp.4.weight', 'transformer.layers.2.prev_layer_weights', 'transformer.layers.3.attention.in_proj_qk.bias', 'transformer.layers.3.attention.in_proj_qk.weight', 'transformer.layers.3.attention.in_proj_v.bias', 'transformer.layers.3.attention.in_proj_v.weight', 'transformer.layers.3.attention.out_proj.bias', 'transformer.layers.3.attention.out_proj.weight', 'transformer.layers.3.attention.position_indices', 'transformer.layers.3.attention.post_layer_norm.bias', 'transformer.layers.3.attention.post_layer_norm.weight', 'transformer.layers.3.mlp.mlp.1.weight', 'transformer.layers.3.mlp.mlp.4.weight', 'transformer.layers.3.prev_layer_weights', 'transformer.layers.4.attention.in_proj_qk.bias', 'transformer.layers.4.attention.in_proj_qk.weight', 'transformer.layers.4.attention.in_proj_v.bias', 'transformer.layers.4.attention.in_proj_v.weight', 'transformer.layers.4.attention.out_proj.bias', 'transformer.layers.4.attention.out_proj.weight', 'transformer.layers.4.attention.position_indices', 'transformer.layers.4.attention.post_layer_norm.bias', 'transformer.layers.4.attention.post_layer_norm.weight', 'transformer.layers.4.mlp.mlp.1.weight', 'transformer.layers.4.mlp.mlp.4.weight', 'transformer.layers.4.prev_layer_weights', 'transformer.layers.5.attention.in_proj_qk.bias', 'transformer.layers.5.attention.in_proj_qk.weight', 'transformer.layers.5.attention.in_proj_v.bias', 'transformer.layers.5.attention.in_proj_v.weight', 'transformer.layers.5.attention.out_proj.bias', 'transformer.layers.5.attention.out_proj.weight', 'transformer.layers.5.attention.position_indices', 'transformer.layers.5.attention.post_layer_norm.bias', 'transformer.layers.5.attention.post_layer_norm.weight', 'transformer.layers.5.mlp.mlp.1.weight', 'transformer.layers.5.mlp.mlp.4.weight', 'transformer.layers.5.prev_layer_weights', 'transformer.layers.6.attention.in_proj_qk.bias', 'transformer.layers.6.attention.in_proj_qk.weight', 'transformer.layers.6.attention.in_proj_v.bias', 'transformer.layers.6.attention.in_proj_v.weight', 'transformer.layers.6.attention.out_proj.bias', 'transformer.layers.6.attention.out_proj.weight', 'transformer.layers.6.attention.position_indices', 'transformer.layers.6.attention.post_layer_norm.bias', 'transformer.layers.6.attention.post_layer_norm.weight', 'transformer.layers.6.mlp.mlp.1.weight', 'transformer.layers.6.mlp.mlp.4.weight', 'transformer.layers.6.prev_layer_weights', 'transformer.layers.7.attention.in_proj_qk.bias', 'transformer.layers.7.attention.in_proj_qk.weight', 'transformer.layers.7.attention.in_proj_v.bias', 'transformer.layers.7.attention.in_proj_v.weight', 'transformer.layers.7.attention.out_proj.bias', 'transformer.layers.7.attention.out_proj.weight', 'transformer.layers.7.attention.position_indices', 'transformer.layers.7.attention.post_layer_norm.bias', 'transformer.layers.7.attention.post_layer_norm.weight', 'transformer.layers.7.mlp.mlp.1.weight', 'transformer.layers.7.mlp.mlp.4.weight', 'transformer.layers.7.prev_layer_weights', 'transformer.layers.8.attention.in_proj_qk.bias', 'transformer.layers.8.attention.in_proj_qk.weight', 'transformer.layers.8.attention.in_proj_v.bias', 'transformer.layers.8.attention.in_proj_v.weight', 'transformer.layers.8.attention.out_proj.bias', 'transformer.layers.8.attention.out_proj.weight', 'transformer.layers.8.attention.position_indices', 'transformer.layers.8.attention.post_layer_norm.bias', 'transformer.layers.8.attention.post_layer_norm.weight', 'transformer.layers.8.mlp.mlp.1.weight', 'transformer.layers.8.mlp.mlp.4.weight', 'transformer.layers.8.prev_layer_weights', 'transformer.layers.9.attention.in_proj_qk.bias', 'transformer.layers.9.attention.in_proj_qk.weight', 'transformer.layers.9.attention.in_proj_v.bias', 'transformer.layers.9.attention.in_proj_v.weight', 'transformer.layers.9.attention.out_proj.bias', 'transformer.layers.9.attention.out_proj.weight', 'transformer.layers.9.attention.position_indices', 'transformer.layers.9.attention.post_layer_norm.bias', 'transformer.layers.9.attention.post_layer_norm.weight', 'transformer.layers.9.mlp.mlp.1.weight', 'transformer.layers.9.mlp.mlp.4.weight', 'transformer.layers.9.prev_layer_weights']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LtgBertForSequenceClassification(\n",
            "  (embedding): Embedding(\n",
            "    (word_embedding): Embedding(6144, 384)\n",
            "    (word_layer_norm): LayerNorm((384,), eps=1e-07, elementwise_affine=False)\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "    (relative_layer_norm): LayerNorm((384,), eps=1e-07, elementwise_affine=True)\n",
            "  )\n",
            "  (transformer): Encoder(\n",
            "    (layers): ModuleList(\n",
            "      (0-11): 12 x EncoderLayer(\n",
            "        (attention): Attention(\n",
            "          (in_proj_qk): Linear(in_features=384, out_features=768, bias=True)\n",
            "          (in_proj_v): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (pre_layer_norm): LayerNorm((384,), eps=1e-07, elementwise_affine=False)\n",
            "          (post_layer_norm): LayerNorm((384,), eps=1e-07, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (mlp): FeedForward(\n",
            "          (mlp): Sequential(\n",
            "            (0): LayerNorm((384,), eps=1e-07, elementwise_affine=False)\n",
            "            (1): Linear(in_features=384, out_features=2048, bias=False)\n",
            "            (2): GeGLU()\n",
            "            (3): LayerNorm((1024,), eps=1e-07, elementwise_affine=False)\n",
            "            (4): Linear(in_features=1024, out_features=384, bias=False)\n",
            "            (5): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (head): Classifier(\n",
            "    (nonlinearity): Sequential(\n",
            "      (0): LayerNorm((384,), eps=1e-07, elementwise_affine=False)\n",
            "      (1): Linear(in_features=384, out_features=384, bias=True)\n",
            "      (2): GELU(approximate='none')\n",
            "      (3): LayerNorm((384,), eps=1e-07, elementwise_affine=False)\n",
            "      (4): Dropout(p=0.2, inplace=False)\n",
            "      (5): Linear(in_features=384, out_features=2, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls -lh /content/elc-bert-replica"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hmc9rukDzxzs",
        "outputId": "1f7e8fd6-dfb9-4958-c8fe-6157f125d5de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 594M\n",
            "-rw-r--r--  1 root root  759 May 11 18:19 config.json\n",
            "-rw-r--r--  1 root root 5.5K May 11 18:19 configuration_ltgbert.py\n",
            "drwxr-xr-x 24 root root 4.0K May 11 18:44 \u001b[0m\u001b[01;34mfinetune\u001b[0m/\n",
            "-rw-r--r--  1 root root 297M May 11 18:19 model.bin\n",
            "-rw-r--r--  1 root root  35K May 11 18:19 modeling_ltgbert.py\n",
            "-rw-r--r--  1 root root 297M May 11 18:19 pytorch_model.bin\n",
            "-rw-r--r--  1 root root  173 May 11 18:19 special_tokens_map.json\n",
            "-rw-r--r--  1 root root  106 May 11 18:19 tokenizer_config.json\n",
            "-rw-r--r--  1 root root 132K May 11 18:19 tokenizer.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.serialization import add_safe_globals\n",
        "from argparse import Namespace\n",
        "\n",
        "add_safe_globals([Namespace])\n",
        "\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"/content/elc-bert-replica\",\n",
        "    trust_remote_code=True,\n",
        "    ignore_mismatched_sizes=True,\n",
        "    weights_only=True,\n",
        ")\n",
        "\n",
        "print(\"\\n=== Parameter Device Check ===\")\n",
        "for name, param in model.named_parameters():\n",
        "    print(f\"{name}: {param.device} | requires_grad={param.requires_grad}\")\n",
        "\n",
        "print(\"\\n=== Buffer Device Check ===\")\n",
        "for name, buffer in model.named_buffers():\n",
        "    print(f\"{name}: {buffer.device}\")\n",
        "\n",
        "print(\"\\nModel successfully loaded without meta tensors if no 'meta' devices are shown above.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rspr0FbX1ZW2",
        "outputId": "5027c94f-cf6f-45c7-ca95-d344a7295cdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of LtgBertForSequenceClassification were not initialized from the model checkpoint at /content/elc-bert-replica and are newly initialized: ['embedding.relative_embedding', 'embedding.relative_layer_norm.bias', 'embedding.relative_layer_norm.weight', 'embedding.word_embedding.weight', 'transformer.layers.0.attention.in_proj_qk.bias', 'transformer.layers.0.attention.in_proj_qk.weight', 'transformer.layers.0.attention.in_proj_v.bias', 'transformer.layers.0.attention.in_proj_v.weight', 'transformer.layers.0.attention.out_proj.bias', 'transformer.layers.0.attention.out_proj.weight', 'transformer.layers.0.attention.position_indices', 'transformer.layers.0.attention.post_layer_norm.bias', 'transformer.layers.0.attention.post_layer_norm.weight', 'transformer.layers.0.mlp.mlp.1.weight', 'transformer.layers.0.mlp.mlp.4.weight', 'transformer.layers.0.prev_layer_weights', 'transformer.layers.1.attention.in_proj_qk.bias', 'transformer.layers.1.attention.in_proj_qk.weight', 'transformer.layers.1.attention.in_proj_v.bias', 'transformer.layers.1.attention.in_proj_v.weight', 'transformer.layers.1.attention.out_proj.bias', 'transformer.layers.1.attention.out_proj.weight', 'transformer.layers.1.attention.position_indices', 'transformer.layers.1.attention.post_layer_norm.bias', 'transformer.layers.1.attention.post_layer_norm.weight', 'transformer.layers.1.mlp.mlp.1.weight', 'transformer.layers.1.mlp.mlp.4.weight', 'transformer.layers.1.prev_layer_weights', 'transformer.layers.10.attention.in_proj_qk.bias', 'transformer.layers.10.attention.in_proj_qk.weight', 'transformer.layers.10.attention.in_proj_v.bias', 'transformer.layers.10.attention.in_proj_v.weight', 'transformer.layers.10.attention.out_proj.bias', 'transformer.layers.10.attention.out_proj.weight', 'transformer.layers.10.attention.position_indices', 'transformer.layers.10.attention.post_layer_norm.bias', 'transformer.layers.10.attention.post_layer_norm.weight', 'transformer.layers.10.mlp.mlp.1.weight', 'transformer.layers.10.mlp.mlp.4.weight', 'transformer.layers.10.prev_layer_weights', 'transformer.layers.11.attention.in_proj_qk.bias', 'transformer.layers.11.attention.in_proj_qk.weight', 'transformer.layers.11.attention.in_proj_v.bias', 'transformer.layers.11.attention.in_proj_v.weight', 'transformer.layers.11.attention.out_proj.bias', 'transformer.layers.11.attention.out_proj.weight', 'transformer.layers.11.attention.position_indices', 'transformer.layers.11.attention.post_layer_norm.bias', 'transformer.layers.11.attention.post_layer_norm.weight', 'transformer.layers.11.mlp.mlp.1.weight', 'transformer.layers.11.mlp.mlp.4.weight', 'transformer.layers.11.prev_layer_weights', 'transformer.layers.2.attention.in_proj_qk.bias', 'transformer.layers.2.attention.in_proj_qk.weight', 'transformer.layers.2.attention.in_proj_v.bias', 'transformer.layers.2.attention.in_proj_v.weight', 'transformer.layers.2.attention.out_proj.bias', 'transformer.layers.2.attention.out_proj.weight', 'transformer.layers.2.attention.position_indices', 'transformer.layers.2.attention.post_layer_norm.bias', 'transformer.layers.2.attention.post_layer_norm.weight', 'transformer.layers.2.mlp.mlp.1.weight', 'transformer.layers.2.mlp.mlp.4.weight', 'transformer.layers.2.prev_layer_weights', 'transformer.layers.3.attention.in_proj_qk.bias', 'transformer.layers.3.attention.in_proj_qk.weight', 'transformer.layers.3.attention.in_proj_v.bias', 'transformer.layers.3.attention.in_proj_v.weight', 'transformer.layers.3.attention.out_proj.bias', 'transformer.layers.3.attention.out_proj.weight', 'transformer.layers.3.attention.position_indices', 'transformer.layers.3.attention.post_layer_norm.bias', 'transformer.layers.3.attention.post_layer_norm.weight', 'transformer.layers.3.mlp.mlp.1.weight', 'transformer.layers.3.mlp.mlp.4.weight', 'transformer.layers.3.prev_layer_weights', 'transformer.layers.4.attention.in_proj_qk.bias', 'transformer.layers.4.attention.in_proj_qk.weight', 'transformer.layers.4.attention.in_proj_v.bias', 'transformer.layers.4.attention.in_proj_v.weight', 'transformer.layers.4.attention.out_proj.bias', 'transformer.layers.4.attention.out_proj.weight', 'transformer.layers.4.attention.position_indices', 'transformer.layers.4.attention.post_layer_norm.bias', 'transformer.layers.4.attention.post_layer_norm.weight', 'transformer.layers.4.mlp.mlp.1.weight', 'transformer.layers.4.mlp.mlp.4.weight', 'transformer.layers.4.prev_layer_weights', 'transformer.layers.5.attention.in_proj_qk.bias', 'transformer.layers.5.attention.in_proj_qk.weight', 'transformer.layers.5.attention.in_proj_v.bias', 'transformer.layers.5.attention.in_proj_v.weight', 'transformer.layers.5.attention.out_proj.bias', 'transformer.layers.5.attention.out_proj.weight', 'transformer.layers.5.attention.position_indices', 'transformer.layers.5.attention.post_layer_norm.bias', 'transformer.layers.5.attention.post_layer_norm.weight', 'transformer.layers.5.mlp.mlp.1.weight', 'transformer.layers.5.mlp.mlp.4.weight', 'transformer.layers.5.prev_layer_weights', 'transformer.layers.6.attention.in_proj_qk.bias', 'transformer.layers.6.attention.in_proj_qk.weight', 'transformer.layers.6.attention.in_proj_v.bias', 'transformer.layers.6.attention.in_proj_v.weight', 'transformer.layers.6.attention.out_proj.bias', 'transformer.layers.6.attention.out_proj.weight', 'transformer.layers.6.attention.position_indices', 'transformer.layers.6.attention.post_layer_norm.bias', 'transformer.layers.6.attention.post_layer_norm.weight', 'transformer.layers.6.mlp.mlp.1.weight', 'transformer.layers.6.mlp.mlp.4.weight', 'transformer.layers.6.prev_layer_weights', 'transformer.layers.7.attention.in_proj_qk.bias', 'transformer.layers.7.attention.in_proj_qk.weight', 'transformer.layers.7.attention.in_proj_v.bias', 'transformer.layers.7.attention.in_proj_v.weight', 'transformer.layers.7.attention.out_proj.bias', 'transformer.layers.7.attention.out_proj.weight', 'transformer.layers.7.attention.position_indices', 'transformer.layers.7.attention.post_layer_norm.bias', 'transformer.layers.7.attention.post_layer_norm.weight', 'transformer.layers.7.mlp.mlp.1.weight', 'transformer.layers.7.mlp.mlp.4.weight', 'transformer.layers.7.prev_layer_weights', 'transformer.layers.8.attention.in_proj_qk.bias', 'transformer.layers.8.attention.in_proj_qk.weight', 'transformer.layers.8.attention.in_proj_v.bias', 'transformer.layers.8.attention.in_proj_v.weight', 'transformer.layers.8.attention.out_proj.bias', 'transformer.layers.8.attention.out_proj.weight', 'transformer.layers.8.attention.position_indices', 'transformer.layers.8.attention.post_layer_norm.bias', 'transformer.layers.8.attention.post_layer_norm.weight', 'transformer.layers.8.mlp.mlp.1.weight', 'transformer.layers.8.mlp.mlp.4.weight', 'transformer.layers.8.prev_layer_weights', 'transformer.layers.9.attention.in_proj_qk.bias', 'transformer.layers.9.attention.in_proj_qk.weight', 'transformer.layers.9.attention.in_proj_v.bias', 'transformer.layers.9.attention.in_proj_v.weight', 'transformer.layers.9.attention.out_proj.bias', 'transformer.layers.9.attention.out_proj.weight', 'transformer.layers.9.attention.position_indices', 'transformer.layers.9.attention.post_layer_norm.bias', 'transformer.layers.9.attention.post_layer_norm.weight', 'transformer.layers.9.mlp.mlp.1.weight', 'transformer.layers.9.mlp.mlp.4.weight', 'transformer.layers.9.prev_layer_weights']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Parameter Device Check ===\n",
            "embedding.relative_embedding: cpu | requires_grad=True\n",
            "embedding.word_embedding.weight: cpu | requires_grad=True\n",
            "embedding.relative_layer_norm.weight: cpu | requires_grad=True\n",
            "embedding.relative_layer_norm.bias: cpu | requires_grad=True\n",
            "transformer.layers.0.prev_layer_weights: cpu | requires_grad=True\n",
            "transformer.layers.0.attention.in_proj_qk.weight: cpu | requires_grad=True\n",
            "transformer.layers.0.attention.in_proj_qk.bias: cpu | requires_grad=True\n",
            "transformer.layers.0.attention.in_proj_v.weight: cpu | requires_grad=True\n",
            "transformer.layers.0.attention.in_proj_v.bias: cpu | requires_grad=True\n",
            "transformer.layers.0.attention.out_proj.weight: cpu | requires_grad=True\n",
            "transformer.layers.0.attention.out_proj.bias: cpu | requires_grad=True\n",
            "transformer.layers.0.attention.post_layer_norm.weight: cpu | requires_grad=True\n",
            "transformer.layers.0.attention.post_layer_norm.bias: cpu | requires_grad=True\n",
            "transformer.layers.0.mlp.mlp.1.weight: cpu | requires_grad=True\n",
            "transformer.layers.0.mlp.mlp.4.weight: cpu | requires_grad=True\n",
            "transformer.layers.1.prev_layer_weights: cpu | requires_grad=True\n",
            "transformer.layers.1.attention.in_proj_qk.weight: cpu | requires_grad=True\n",
            "transformer.layers.1.attention.in_proj_qk.bias: cpu | requires_grad=True\n",
            "transformer.layers.1.attention.in_proj_v.weight: cpu | requires_grad=True\n",
            "transformer.layers.1.attention.in_proj_v.bias: cpu | requires_grad=True\n",
            "transformer.layers.1.attention.out_proj.weight: cpu | requires_grad=True\n",
            "transformer.layers.1.attention.out_proj.bias: cpu | requires_grad=True\n",
            "transformer.layers.1.attention.post_layer_norm.weight: cpu | requires_grad=True\n",
            "transformer.layers.1.attention.post_layer_norm.bias: cpu | requires_grad=True\n",
            "transformer.layers.1.mlp.mlp.1.weight: cpu | requires_grad=True\n",
            "transformer.layers.1.mlp.mlp.4.weight: cpu | requires_grad=True\n",
            "transformer.layers.2.prev_layer_weights: cpu | requires_grad=True\n",
            "transformer.layers.2.attention.in_proj_qk.weight: cpu | requires_grad=True\n",
            "transformer.layers.2.attention.in_proj_qk.bias: cpu | requires_grad=True\n",
            "transformer.layers.2.attention.in_proj_v.weight: cpu | requires_grad=True\n",
            "transformer.layers.2.attention.in_proj_v.bias: cpu | requires_grad=True\n",
            "transformer.layers.2.attention.out_proj.weight: cpu | requires_grad=True\n",
            "transformer.layers.2.attention.out_proj.bias: cpu | requires_grad=True\n",
            "transformer.layers.2.attention.post_layer_norm.weight: cpu | requires_grad=True\n",
            "transformer.layers.2.attention.post_layer_norm.bias: cpu | requires_grad=True\n",
            "transformer.layers.2.mlp.mlp.1.weight: cpu | requires_grad=True\n",
            "transformer.layers.2.mlp.mlp.4.weight: cpu | requires_grad=True\n",
            "transformer.layers.3.prev_layer_weights: cpu | requires_grad=True\n",
            "transformer.layers.3.attention.in_proj_qk.weight: cpu | requires_grad=True\n",
            "transformer.layers.3.attention.in_proj_qk.bias: cpu | requires_grad=True\n",
            "transformer.layers.3.attention.in_proj_v.weight: cpu | requires_grad=True\n",
            "transformer.layers.3.attention.in_proj_v.bias: cpu | requires_grad=True\n",
            "transformer.layers.3.attention.out_proj.weight: cpu | requires_grad=True\n",
            "transformer.layers.3.attention.out_proj.bias: cpu | requires_grad=True\n",
            "transformer.layers.3.attention.post_layer_norm.weight: cpu | requires_grad=True\n",
            "transformer.layers.3.attention.post_layer_norm.bias: cpu | requires_grad=True\n",
            "transformer.layers.3.mlp.mlp.1.weight: cpu | requires_grad=True\n",
            "transformer.layers.3.mlp.mlp.4.weight: cpu | requires_grad=True\n",
            "transformer.layers.4.prev_layer_weights: cpu | requires_grad=True\n",
            "transformer.layers.4.attention.in_proj_qk.weight: cpu | requires_grad=True\n",
            "transformer.layers.4.attention.in_proj_qk.bias: cpu | requires_grad=True\n",
            "transformer.layers.4.attention.in_proj_v.weight: cpu | requires_grad=True\n",
            "transformer.layers.4.attention.in_proj_v.bias: cpu | requires_grad=True\n",
            "transformer.layers.4.attention.out_proj.weight: cpu | requires_grad=True\n",
            "transformer.layers.4.attention.out_proj.bias: cpu | requires_grad=True\n",
            "transformer.layers.4.attention.post_layer_norm.weight: cpu | requires_grad=True\n",
            "transformer.layers.4.attention.post_layer_norm.bias: cpu | requires_grad=True\n",
            "transformer.layers.4.mlp.mlp.1.weight: cpu | requires_grad=True\n",
            "transformer.layers.4.mlp.mlp.4.weight: cpu | requires_grad=True\n",
            "transformer.layers.5.prev_layer_weights: cpu | requires_grad=True\n",
            "transformer.layers.5.attention.in_proj_qk.weight: cpu | requires_grad=True\n",
            "transformer.layers.5.attention.in_proj_qk.bias: cpu | requires_grad=True\n",
            "transformer.layers.5.attention.in_proj_v.weight: cpu | requires_grad=True\n",
            "transformer.layers.5.attention.in_proj_v.bias: cpu | requires_grad=True\n",
            "transformer.layers.5.attention.out_proj.weight: cpu | requires_grad=True\n",
            "transformer.layers.5.attention.out_proj.bias: cpu | requires_grad=True\n",
            "transformer.layers.5.attention.post_layer_norm.weight: cpu | requires_grad=True\n",
            "transformer.layers.5.attention.post_layer_norm.bias: cpu | requires_grad=True\n",
            "transformer.layers.5.mlp.mlp.1.weight: cpu | requires_grad=True\n",
            "transformer.layers.5.mlp.mlp.4.weight: cpu | requires_grad=True\n",
            "transformer.layers.6.prev_layer_weights: cpu | requires_grad=True\n",
            "transformer.layers.6.attention.in_proj_qk.weight: cpu | requires_grad=True\n",
            "transformer.layers.6.attention.in_proj_qk.bias: cpu | requires_grad=True\n",
            "transformer.layers.6.attention.in_proj_v.weight: cpu | requires_grad=True\n",
            "transformer.layers.6.attention.in_proj_v.bias: cpu | requires_grad=True\n",
            "transformer.layers.6.attention.out_proj.weight: cpu | requires_grad=True\n",
            "transformer.layers.6.attention.out_proj.bias: cpu | requires_grad=True\n",
            "transformer.layers.6.attention.post_layer_norm.weight: cpu | requires_grad=True\n",
            "transformer.layers.6.attention.post_layer_norm.bias: cpu | requires_grad=True\n",
            "transformer.layers.6.mlp.mlp.1.weight: cpu | requires_grad=True\n",
            "transformer.layers.6.mlp.mlp.4.weight: cpu | requires_grad=True\n",
            "transformer.layers.7.prev_layer_weights: cpu | requires_grad=True\n",
            "transformer.layers.7.attention.in_proj_qk.weight: cpu | requires_grad=True\n",
            "transformer.layers.7.attention.in_proj_qk.bias: cpu | requires_grad=True\n",
            "transformer.layers.7.attention.in_proj_v.weight: cpu | requires_grad=True\n",
            "transformer.layers.7.attention.in_proj_v.bias: cpu | requires_grad=True\n",
            "transformer.layers.7.attention.out_proj.weight: cpu | requires_grad=True\n",
            "transformer.layers.7.attention.out_proj.bias: cpu | requires_grad=True\n",
            "transformer.layers.7.attention.post_layer_norm.weight: cpu | requires_grad=True\n",
            "transformer.layers.7.attention.post_layer_norm.bias: cpu | requires_grad=True\n",
            "transformer.layers.7.mlp.mlp.1.weight: cpu | requires_grad=True\n",
            "transformer.layers.7.mlp.mlp.4.weight: cpu | requires_grad=True\n",
            "transformer.layers.8.prev_layer_weights: cpu | requires_grad=True\n",
            "transformer.layers.8.attention.in_proj_qk.weight: cpu | requires_grad=True\n",
            "transformer.layers.8.attention.in_proj_qk.bias: cpu | requires_grad=True\n",
            "transformer.layers.8.attention.in_proj_v.weight: cpu | requires_grad=True\n",
            "transformer.layers.8.attention.in_proj_v.bias: cpu | requires_grad=True\n",
            "transformer.layers.8.attention.out_proj.weight: cpu | requires_grad=True\n",
            "transformer.layers.8.attention.out_proj.bias: cpu | requires_grad=True\n",
            "transformer.layers.8.attention.post_layer_norm.weight: cpu | requires_grad=True\n",
            "transformer.layers.8.attention.post_layer_norm.bias: cpu | requires_grad=True\n",
            "transformer.layers.8.mlp.mlp.1.weight: cpu | requires_grad=True\n",
            "transformer.layers.8.mlp.mlp.4.weight: cpu | requires_grad=True\n",
            "transformer.layers.9.prev_layer_weights: cpu | requires_grad=True\n",
            "transformer.layers.9.attention.in_proj_qk.weight: cpu | requires_grad=True\n",
            "transformer.layers.9.attention.in_proj_qk.bias: cpu | requires_grad=True\n",
            "transformer.layers.9.attention.in_proj_v.weight: cpu | requires_grad=True\n",
            "transformer.layers.9.attention.in_proj_v.bias: cpu | requires_grad=True\n",
            "transformer.layers.9.attention.out_proj.weight: cpu | requires_grad=True\n",
            "transformer.layers.9.attention.out_proj.bias: cpu | requires_grad=True\n",
            "transformer.layers.9.attention.post_layer_norm.weight: cpu | requires_grad=True\n",
            "transformer.layers.9.attention.post_layer_norm.bias: cpu | requires_grad=True\n",
            "transformer.layers.9.mlp.mlp.1.weight: cpu | requires_grad=True\n",
            "transformer.layers.9.mlp.mlp.4.weight: cpu | requires_grad=True\n",
            "transformer.layers.10.prev_layer_weights: cpu | requires_grad=True\n",
            "transformer.layers.10.attention.in_proj_qk.weight: cpu | requires_grad=True\n",
            "transformer.layers.10.attention.in_proj_qk.bias: cpu | requires_grad=True\n",
            "transformer.layers.10.attention.in_proj_v.weight: cpu | requires_grad=True\n",
            "transformer.layers.10.attention.in_proj_v.bias: cpu | requires_grad=True\n",
            "transformer.layers.10.attention.out_proj.weight: cpu | requires_grad=True\n",
            "transformer.layers.10.attention.out_proj.bias: cpu | requires_grad=True\n",
            "transformer.layers.10.attention.post_layer_norm.weight: cpu | requires_grad=True\n",
            "transformer.layers.10.attention.post_layer_norm.bias: cpu | requires_grad=True\n",
            "transformer.layers.10.mlp.mlp.1.weight: cpu | requires_grad=True\n",
            "transformer.layers.10.mlp.mlp.4.weight: cpu | requires_grad=True\n",
            "transformer.layers.11.prev_layer_weights: cpu | requires_grad=True\n",
            "transformer.layers.11.attention.in_proj_qk.weight: cpu | requires_grad=True\n",
            "transformer.layers.11.attention.in_proj_qk.bias: cpu | requires_grad=True\n",
            "transformer.layers.11.attention.in_proj_v.weight: cpu | requires_grad=True\n",
            "transformer.layers.11.attention.in_proj_v.bias: cpu | requires_grad=True\n",
            "transformer.layers.11.attention.out_proj.weight: cpu | requires_grad=True\n",
            "transformer.layers.11.attention.out_proj.bias: cpu | requires_grad=True\n",
            "transformer.layers.11.attention.post_layer_norm.weight: cpu | requires_grad=True\n",
            "transformer.layers.11.attention.post_layer_norm.bias: cpu | requires_grad=True\n",
            "transformer.layers.11.mlp.mlp.1.weight: cpu | requires_grad=True\n",
            "transformer.layers.11.mlp.mlp.4.weight: cpu | requires_grad=True\n",
            "head.nonlinearity.1.weight: meta | requires_grad=True\n",
            "head.nonlinearity.1.bias: meta | requires_grad=True\n",
            "head.nonlinearity.5.weight: meta | requires_grad=True\n",
            "head.nonlinearity.5.bias: meta | requires_grad=True\n",
            "\n",
            "=== Buffer Device Check ===\n",
            "transformer.layers.0.attention.position_indices: cpu\n",
            "transformer.layers.1.attention.position_indices: cpu\n",
            "transformer.layers.2.attention.position_indices: cpu\n",
            "transformer.layers.3.attention.position_indices: cpu\n",
            "transformer.layers.4.attention.position_indices: cpu\n",
            "transformer.layers.5.attention.position_indices: cpu\n",
            "transformer.layers.6.attention.position_indices: cpu\n",
            "transformer.layers.7.attention.position_indices: cpu\n",
            "transformer.layers.8.attention.position_indices: cpu\n",
            "transformer.layers.9.attention.position_indices: cpu\n",
            "transformer.layers.10.attention.position_indices: cpu\n",
            "transformer.layers.11.attention.position_indices: cpu\n",
            "\n",
            "Model successfully loaded without meta tensors if no 'meta' devices are shown above.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}